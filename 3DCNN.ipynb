{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from imageio import imread\n",
    "import matplotlib.pyplot as plt\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Flatten, BatchNormalization, Dropout\n",
    "from keras.layers import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar 18 16:30:32 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 522.06       Driver Version: 522.06       CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   54C    P8     1W /  N/A |   2830MiB /  4096MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2532      C   ...conda3\\envs\\ml\\python.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n",
      "GPU found\n"
     ]
    }
   ],
   "source": [
    "## Checking the GPU configuration\n",
    "!nvidia-smi\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "else:\n",
    "    print(\"No GPU found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './dataset_aug'\n",
    "csv_path = './dataset_aug.csv'\n",
    "\n",
    "data = []\n",
    "with open(csv_path, 'r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    for row in csvreader:\n",
    "        data.append(row)\n",
    "\n",
    "data = np.array(data)\n",
    "\n",
    "num_folds = 5\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=34)\n",
    "\n",
    "gesture_num = 6\n",
    "np.random.seed(34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeImage(image):\n",
    "    # applying normalization\n",
    "    return image/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make3dFilter(x):\n",
    "    return tuple([x]*3)\n",
    "\n",
    "def make2dFilter(x):\n",
    "    return tuple([x]*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatchData(source_path, t, batch, batch_size, img_tensor):\n",
    "    [x,y,z] = [len(img_tensor[0]),img_tensor[1], img_tensor[2]]\n",
    "    img_idx = img_tensor[0]\n",
    "    batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "    batch_labels = np.zeros((batch_size, gesture_num)) # batch_labels is the one hot representation of the output\n",
    "    for folder in range(batch_size): # iterate over the batch_size\n",
    "        vid = t[folder + (batch*batch_size)]\n",
    "        vid_path = os.path.join(source_path, os.path.join(vid[1], vid[0]))\n",
    "        imgs = os.listdir(vid_path) # read all the images in the folder\n",
    "        for idx,item in enumerate(img_idx): #  Iterate over the frames/images of a folder to read them in\n",
    "#             print(\"idx: \", idx)\n",
    "#             print(\"item: \",item)\n",
    "            image = imread(os.path.join(vid_path, imgs[item])).astype(np.float32)\n",
    "\n",
    "            #crop the images and resize them. Note that the images are of 2 different shape \n",
    "            #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "\n",
    "            # separate preprocessImage function is defined for cropping, resizing and normalizing images\n",
    "            batch_data[folder,idx,:,:,0] = normalizeImage(image[:, :, 0])\n",
    "            batch_data[folder,idx,:,:,1] = normalizeImage(image[:, :, 1])\n",
    "            batch_data[folder,idx,:,:,2] = normalizeImage(image[:, :, 2])\n",
    "\n",
    "#         print(\"folder: \", folder)\n",
    "#         print(\"index:  \", int(t[folder + (batch*batch_size)].strip().split(',')[2]))\n",
    "        batch_labels[folder, int(vid[2])] = 1\n",
    "    return batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size, img_tensor):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = int(len(folder_list)/batch_size)\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            yield getBatchData(source_path, t, batch, batch_size, img_tensor)\n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        # checking if any remaining batches are there or not\n",
    "        if len(folder_list)%batch_size != 0:\n",
    "            # updated the batch size and yield\n",
    "            batch_size = len(folder_list)%batch_size\n",
    "            yield getBatchData(source_path, t, batch, batch_size, img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImgTensor(n_frames):\n",
    "    img_idx = np.round(np.linspace(0, 29, n_frames)).astype(int)\n",
    "    return [img_idx, 90, 160, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotModelHistory(fold_results):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15,4))\n",
    "\n",
    "    total_val_loss = 0\n",
    "    total_val_acc = 0\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "\n",
    "    for i, fold_result in enumerate(fold_results):\n",
    "        total_val_loss += fold_results[i]['val_loss']\n",
    "        total_val_acc += fold_results[i]['val_accuracy']\n",
    "\n",
    "        h = fold_result['history']\n",
    "        train_loss.append(h.history['loss'])\n",
    "        val_loss.append(h.history['val_loss'])\n",
    "        train_acc.append(h.history['categorical_accuracy'])\n",
    "        val_acc.append(h.history['val_categorical_accuracy'])\n",
    "\n",
    "    avg_train_loss = np.mean(train_loss, axis=0)\n",
    "    avg_val_loss = np.mean(val_loss, axis=0)\n",
    "    std_train_loss = np.std(train_loss, axis=0)\n",
    "    std_val_loss = np.std(val_loss, axis=0)\n",
    "\n",
    "    ax[0].plot(range(1, len(avg_train_loss) + 1), avg_train_loss)   \n",
    "    ax[0].plot(range(1, len(avg_val_loss) + 1), avg_val_loss)\n",
    "    ax[0].fill_between(range(1, len(avg_train_loss) + 1), avg_train_loss - std_train_loss, avg_train_loss + std_train_loss, alpha=0.2, color='blue')\n",
    "    ax[0].fill_between(range(1, len(avg_val_loss) + 1), avg_val_loss - std_val_loss, avg_val_loss + std_val_loss, alpha=0.2, color='orange')\n",
    "    ax[0].legend(['loss','val_loss'])\n",
    "    ax[0].title.set_text(\"Train loss vs Validation loss\")\n",
    "\n",
    "    avg_train_acc = np.mean(train_acc, axis=0)\n",
    "    avg_val_acc = np.mean(val_acc, axis=0)\n",
    "    std_train_acc = np.std(train_acc, axis=0)\n",
    "    std_val_acc = np.std(val_acc, axis=0)\n",
    "\n",
    "    ax[1].plot(range(1, len(avg_train_acc) + 1), avg_train_acc)   \n",
    "    ax[1].plot(range(1, len(avg_val_acc) + 1), avg_val_acc)\n",
    "    ax[1].fill_between(range(1, len(avg_train_acc) + 1), avg_train_acc - std_train_acc, avg_train_acc + std_train_acc, alpha=0.2, color='blue')\n",
    "    ax[1].fill_between(range(1, len(avg_val_acc) + 1), avg_val_acc - std_val_acc, avg_val_acc + std_val_acc, alpha=0.2, color='orange')\n",
    "    ax[1].legend(['categorical_accuracy','val_categorical_accuracy'])\n",
    "    ax[1].title.set_text(\"Train accuracy vs Validation accuracy\")\n",
    "    \n",
    "    plt.show()\n",
    "    print(f\"Mean Validation Loss: {total_val_loss / len(fold_results)}\")\n",
    "    print(f\"Mean Validation Accuracy: {total_val_acc / len(fold_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"conv_3d1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_18 (Conv3D)          (None, 11, 86, 156, 16)   6016      \n",
      "                                                                 \n",
      " max_pooling3d_18 (MaxPoolin  (None, 6, 43, 78, 16)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_30 (Bat  (None, 6, 43, 78, 16)    64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv3d_19 (Conv3D)          (None, 4, 41, 76, 32)     13856     \n",
      "                                                                 \n",
      " max_pooling3d_19 (MaxPoolin  (None, 4, 21, 38, 32)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_31 (Bat  (None, 4, 21, 38, 32)    128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv3d_20 (Conv3D)          (None, 2, 19, 36, 64)     55360     \n",
      "                                                                 \n",
      " max_pooling3d_20 (MaxPoolin  (None, 2, 10, 18, 64)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_32 (Bat  (None, 2, 10, 18, 64)    256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 23040)             0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 128)               2949248   \n",
      "                                                                 \n",
      " batch_normalization_33 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_34 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,034,342\n",
      "Trainable params: 3,033,734\n",
      "Non-trainable params: 608\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "n_frames = 15\n",
    "num_epochs = 30\n",
    "batch_size = 8\n",
    "\n",
    "img_tensor = getImgTensor(n_frames)\n",
    "    \n",
    "inputShape = (len(img_tensor[0]), img_tensor[1], img_tensor[2], img_tensor[3])\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n",
    "callbacks_list = [LR]\n",
    "\n",
    "model1 = Sequential([\n",
    "    Conv3D(16, make3dFilter(5), activation='relu', input_shape=inputShape),\n",
    "    MaxPooling3D(make3dFilter(2), padding='same'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv3D(32, make3dFilter(3), activation='relu'),\n",
    "    MaxPooling3D(pool_size=(1,2,2), padding='same'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv3D(64, make3dFilter(3), activation='relu'),\n",
    "    MaxPooling3D(pool_size=(1,2,2), padding='same'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Dense(gesture_num, activation='softmax')\n",
    "], name=\"conv_3d1\")\n",
    "model1.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print(model1.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold:  0\n",
      "# training sequences = 960\n",
      "# validation sequences = 240\n",
      "Source path =  ./dataset_aug ; batch size = 8\n",
      "Epoch 1/30\n",
      "120/120 [==============================] - ETA: 0s - loss: 2.1252 - categorical_accuracy: 0.2646Source path =  ./dataset_aug ; batch size = 8\n",
      "120/120 [==============================] - 62s 511ms/step - loss: 2.1252 - categorical_accuracy: 0.2646 - val_loss: 6.4214 - val_categorical_accuracy: 0.1583 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "120/120 [==============================] - 45s 376ms/step - loss: 1.8186 - categorical_accuracy: 0.3229 - val_loss: 1.6741 - val_categorical_accuracy: 0.3500 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "120/120 [==============================] - 42s 355ms/step - loss: 1.6584 - categorical_accuracy: 0.3615 - val_loss: 2.1228 - val_categorical_accuracy: 0.3042 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "120/120 [==============================] - 46s 385ms/step - loss: 1.5895 - categorical_accuracy: 0.3896 - val_loss: 1.8974 - val_categorical_accuracy: 0.3167 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "120/120 [==============================] - 45s 373ms/step - loss: 1.4346 - categorical_accuracy: 0.4510 - val_loss: 1.3493 - val_categorical_accuracy: 0.4875 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "120/120 [==============================] - 45s 373ms/step - loss: 1.3627 - categorical_accuracy: 0.4979 - val_loss: 1.2082 - val_categorical_accuracy: 0.5750 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "120/120 [==============================] - 49s 414ms/step - loss: 1.2496 - categorical_accuracy: 0.5188 - val_loss: 1.1825 - val_categorical_accuracy: 0.5458 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "120/120 [==============================] - 47s 390ms/step - loss: 1.1530 - categorical_accuracy: 0.5688 - val_loss: 1.0109 - val_categorical_accuracy: 0.6542 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "120/120 [==============================] - 51s 427ms/step - loss: 1.0712 - categorical_accuracy: 0.6115 - val_loss: 1.1168 - val_categorical_accuracy: 0.6083 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "120/120 [==============================] - 47s 393ms/step - loss: 1.1761 - categorical_accuracy: 0.5688 - val_loss: 1.2658 - val_categorical_accuracy: 0.4958 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "120/120 [==============================] - 48s 403ms/step - loss: 0.8958 - categorical_accuracy: 0.6781 - val_loss: 0.8937 - val_categorical_accuracy: 0.6583 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "120/120 [==============================] - 47s 391ms/step - loss: 0.8686 - categorical_accuracy: 0.6677 - val_loss: 0.8240 - val_categorical_accuracy: 0.6875 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "120/120 [==============================] - 46s 382ms/step - loss: 0.8407 - categorical_accuracy: 0.6969 - val_loss: 1.2106 - val_categorical_accuracy: 0.5542 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "120/120 [==============================] - 45s 378ms/step - loss: 0.7342 - categorical_accuracy: 0.7417 - val_loss: 0.9049 - val_categorical_accuracy: 0.6792 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "120/120 [==============================] - 46s 386ms/step - loss: 0.6910 - categorical_accuracy: 0.7552 - val_loss: 0.8136 - val_categorical_accuracy: 0.6833 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "120/120 [==============================] - 46s 385ms/step - loss: 0.7299 - categorical_accuracy: 0.7479 - val_loss: 1.1181 - val_categorical_accuracy: 0.6375 - lr: 0.0010\n",
      "Epoch 17/30\n",
      "120/120 [==============================] - 56s 468ms/step - loss: 0.7716 - categorical_accuracy: 0.7094 - val_loss: 2.3944 - val_categorical_accuracy: 0.4083 - lr: 0.0010\n",
      "Epoch 18/30\n",
      "  3/120 [..............................] - ETA: 36s - loss: 0.7272 - categorical_accuracy: 0.7083"
     ]
    }
   ],
   "source": [
    "fold_results1 = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(data, data[:, 2])):\n",
    "    print(\"fold: \", fold)\n",
    "    train_doc, val_doc = data[train_index], data[val_index]\n",
    "    \n",
    "    train_generator = generator(data_path, train_doc, batch_size, img_tensor)\n",
    "    val_generator = generator(data_path, val_doc, batch_size, img_tensor)\n",
    "\n",
    "    num_train_sequences = len(train_doc)\n",
    "    print('# training sequences =', num_train_sequences)\n",
    "    num_val_sequences = len(val_doc)\n",
    "    print('# validation sequences =', num_val_sequences)\n",
    "\n",
    "    if (num_train_sequences%batch_size) == 0:\n",
    "        steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "    else:\n",
    "        steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "    if (num_val_sequences%batch_size) == 0:\n",
    "        validation_steps = int(num_val_sequences/batch_size)\n",
    "    else:\n",
    "        validation_steps = (num_val_sequences//batch_size) + 1\n",
    "\n",
    "    model1_history = model1.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                callbacks=callbacks_list, validation_data=val_generator, \n",
    "                validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
    "\n",
    "    validation_results = model1.evaluate_generator(val_generator, steps=validation_steps)\n",
    "    print(\"Validation Loss:\", validation_results[0])\n",
    "    print(\"Validation Accuracy:\", validation_results[1])\n",
    "\n",
    "    fold_results1.append({\n",
    "        'history': model1_history,\n",
    "        'val_loss': validation_results[0],\n",
    "        'val_accuracy': validation_results[1]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotModelHistory(fold_results1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frames = 30\n",
    "num_epochs = 30\n",
    "batch_size = 8\n",
    "\n",
    "img_tensor = getImgTensor(n_frames)\n",
    "    \n",
    "inputShape = (len(img_tensor[0]), img_tensor[1], img_tensor[2], img_tensor[3])\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n",
    "callbacks_list = [LR]\n",
    "\n",
    "model2 = Sequential([\n",
    "    Conv3D(16, make3dFilter(5), activation='relu', input_shape=inputShape),\n",
    "    MaxPooling3D(make3dFilter(2), padding='same'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv3D(32, make3dFilter(3), activation='relu'),\n",
    "    MaxPooling3D(pool_size=(1,2,2), padding='same'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv3D(64, make3dFilter(3), activation='relu'),\n",
    "    MaxPooling3D(pool_size=(1,2,2), padding='same'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Dense(gesture_num, activation='softmax')\n",
    "], name=\"conv_3d2\")\n",
    "model2.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print(model2.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_results2 = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(data, data[:, 2])):\n",
    "    print(\"fold: \", fold)\n",
    "    train_doc, val_doc = data[train_index], data[val_index]\n",
    "    \n",
    "    train_generator = generator(data_path, train_doc, batch_size, img_tensor)\n",
    "    val_generator = generator(data_path, val_doc, batch_size, img_tensor)\n",
    "\n",
    "    num_train_sequences = len(train_doc)\n",
    "    print('# training sequences =', num_train_sequences)\n",
    "    num_val_sequences = len(val_doc)\n",
    "    print('# validation sequences =', num_val_sequences)\n",
    "\n",
    "    if (num_train_sequences%batch_size) == 0:\n",
    "        steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "    else:\n",
    "        steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "    if (num_val_sequences%batch_size) == 0:\n",
    "        validation_steps = int(num_val_sequences/batch_size)\n",
    "    else:\n",
    "        validation_steps = (num_val_sequences//batch_size) + 1\n",
    "\n",
    "    model1_history = model2.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                callbacks=callbacks_list, validation_data=val_generator, \n",
    "                validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
    "\n",
    "    validation_results = model2.evaluate_generator(val_generator, steps=validation_steps)\n",
    "    print(\"Validation Loss:\", validation_results[0])\n",
    "    print(\"Validation Accuracy:\", validation_results[1])\n",
    "\n",
    "    fold_results2.append({\n",
    "        'history': model1_history,\n",
    "        'val_loss': validation_results[0],\n",
    "        'val_accuracy': validation_results[1]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotModelHistory(fold_results2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
